{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8697351-e523-4e77-afc1-6ae76c63f0a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Data set/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m path_validate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData set/test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create a dataset using torchvision ImageFolder\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m validate_dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mImageFolder(\n\u001b[0;32m     26\u001b[0m     root\u001b[38;5;241m=\u001b[39mpath_validate,\n\u001b[0;32m     27\u001b[0m     transform\u001b[38;5;241m=\u001b[39mdata_transform,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Split the train_dataset into training and validation subsets\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Data set/train'"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "# Define the data transformations\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Specify the paths to your train and validation datasets\n",
    "path_train = \"Data set/train\"\n",
    "path_validate = \"Data set/test\"\n",
    "\n",
    "# Create a dataset using torchvision ImageFolder\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=path_train,\n",
    "    transform=data_transform,\n",
    ")\n",
    "\n",
    "validate_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=path_validate,\n",
    "    transform=data_transform,\n",
    ")\n",
    "\n",
    "# Split the train_dataset into training and validation subsets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "validate_size = len(train_dataset) - train_size\n",
    "train_dataset, validate_dataset = random_split(train_dataset, [train_size, validate_size])\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=batch_size)\n",
    "\n",
    "# Get the class names\n",
    "class_names = train_dataset.dataset.classes\n",
    "class_name = {i + 1: class_names[i] for i in range(len(class_names))}\n",
    "\n",
    "print(class_names)\n",
    "print(class_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a1fff-9e66-4a67-a9c7-88a291cd224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count = 0  # Initialize a count to keep track of how many images have been plotted\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    for i in range(len(images)):  # Iterate over all available images in the batch\n",
    "        if count >= 9:\n",
    "            break  # Exit the loop if you've plotted 9 images\n",
    "        ax = plt.subplot(3, 3, count + 1)\n",
    "\n",
    "        # Ensure that the image data is in the correct format (e.g., HWC for PyTorch)\n",
    "        image = images[i].permute(1, 2, 0).numpy()\n",
    "\n",
    "        # If your data is not in the range [0, 1], you may need to adjust the scaling\n",
    "        plt.imshow(image)\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "        count += 1\n",
    "    if count >= 9:\n",
    "        break  # Exit the outer loop if you've plotted 9 images\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7be4e-57dc-4fb4-aa1d-a8b006e60d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, img_height, img_width):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.maxpool3 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * (img_height // 8) * (img_width // 8), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the input shape and number of classes\n",
    "# img_height, img_width, num_classes = 224, 224, 10  # Adjust the values as needed\n",
    "\n",
    "# Create the model\n",
    "model = Net(num_classes, img_height, img_width)\n",
    "\n",
    "print(f\"No of Classes : {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b76670-6f40-4a60-80f2-1035c6b48eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define your PyTorch model (assuming you have already defined it)\n",
    "# model = Net()  # Instantiate your model\n",
    "model = Net(num_classes, img_height, img_width) # Instantiate your model\n",
    "\n",
    "# Define the optimizer (e.g., Adam)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define the loss function (e.g., Sparse Categorical Cross-Entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optionally, you can also specify any additional metrics you want to track during training.\n",
    "\n",
    "# Example metric function (accuracy)\n",
    "def accuracy(output, target):\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    correct = (predicted == target).sum().item()\n",
    "    total = target.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# This is just an example; you can add more metrics as needed.\n",
    "\n",
    "# You can then use these objects during your training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe1fe7-7530-4605-84f5-43798fceb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(model, input_size):\n",
    "    print(model)\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Trainable Parameters: {total_params}\")\n",
    "    print(f\"Input Size: {input_size}\")\n",
    "\n",
    "# Example usage of the summary function\n",
    "input_size = (3, img_height, img_width)  # Adjust the input size based on your data\n",
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56218cd5-a1b5-43a9-b810-427bda48fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming you've already defined your model, optimizer, loss function, and data loaders\n",
    "\n",
    "# Specify the number of epochs\n",
    "epochs = 30\n",
    "\n",
    "# Define a function for training\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            total_samples += labels.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate accuracy and print the average loss for the current epoch\n",
    "        accuracy = 100.0 * correct_predictions / total_samples\n",
    "        average_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {average_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "# Train the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assuming you have defined train_loader\n",
    "train_model(model, train_loader, optimizer, criterion, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a1f21-e5a5-47ef-a494-50b863db4e27",
   "metadata": {},
   "source": [
    "###  Predict New Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731937a-bc33-4224-b055-5ebae3137d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4aa79f-60ca-4593-b71d-486513694ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data set/data description.csv\")\n",
    "df = df.replace('',np.nan)\n",
    "df = df.dropna(axis=\"rows\",how=\"all\")\n",
    "df = df.dropna(axis=\"columns\",how=\"all\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51203c-a079-4653-b4f6-9141a6af7c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff18c50d-6c71-4699-a64c-a9a62fb919f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Path to the image\n",
    "path = \"Data set/download.jpeg\"\n",
    "\n",
    "# Load and display the image using PIL and Matplotlib\n",
    "img = Image.open(path)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3f551-905e-47e5-a3c3-52e1e58d6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Path to the image\n",
    "# path = \"mango_dataset/test/Healthy/healthy_146.jpg\"\n",
    "\n",
    "# Define transformations and preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load and preprocess the image, then remove alpha channel if it exists\n",
    "img = Image.open(path).convert('RGB')\n",
    "img = preprocess(img)\n",
    "img = img.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Make a prediction with your PyTorch model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    prediction = model(img)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(prediction).item()\n",
    "\n",
    "Disease\t,Description\t,Symptoms\t,Diagnosis\t,Precaution\t,Medicine_for_cure\t,Stage\t,Severity\t,Recommended_treatment\t,Remedy_to_cure\t = df.loc[predicted_class,:]\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Predicted disease: {class_names[predicted_class]}\")\n",
    "\n",
    "print(f\"Disease : {Disease}\\nDescription : {Description}\\nSymptoms : {Symptoms}\\nDiagnosis : {Diagnosis}\\nPrecaution : {Precaution}\\nSeverity : {Severity}\\nRecommended_treatment : {Recommended_treatment}\\nRemedy_to_cure : {Remedy_to_cure}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793519e-b6f9-4f40-80e0-546d9e30292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'CNN_model123.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aff9f4-4f79-43be-b5fb-ffeee3e387c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac4a4b-500d-4a3d-85a5-736dd0ad1c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
